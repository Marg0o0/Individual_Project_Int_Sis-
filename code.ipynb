{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c0843bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  numpy as np\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from  sklearn.model_selection import KFold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "977c72ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRE-PROCESSAMENTO\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),         #garante que as imagens são a preto e branco\n",
    "    transforms.Resize((48, 48)),    #garante que têm dimensão 48x48\n",
    "    transforms.ToTensor(),          #converte para tensores pytorch (1,48,48)\n",
    "    transforms.Normalize((0.5,), (0.5,))  # normaliza pixels entre -1 e 1 (mean = 0.5, std = 0.5)\n",
    "])\n",
    "\n",
    "# \"SPLIT\" DATASET (definimos o treino e o teste):\n",
    "#ImageFolder → usa os nomes das pastas como labels\n",
    "train_dataset = datasets.ImageFolder( root=\"fer2013/train\", transform=transform)\n",
    "test_dataset = datasets.ImageFolder( root=\"fer2013/test\", transform=transform)\n",
    "\n",
    "#LOAD data\n",
    "test = DataLoader( test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb67832",
   "metadata": {},
   "source": [
    "labels correspondem ás emoçoes ou seja\n",
    "\n",
    "imagens -> x\n",
    "\n",
    "labels -> y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee76520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 48, 48])\n",
      "torch.Size([64])\n",
      "tensor(-1.) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "#VERIFICAR SE ESTÁ TUDO OK\n",
    "images, labels = next(iter(train)) #AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
    "print(images.shape)  # deve dar: [64, 1, 48, 48]\n",
    "print(labels.shape)  # deve dar: [64]\n",
    "print(images.min(), images.max())  # já está normalizado para -1 a 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6472f04d",
   "metadata": {},
   "source": [
    "0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3740a3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels values (y): \n",
      " tensor([6, 3, 5, 5, 0, 3, 3, 3, 2, 5, 0, 3, 3, 3, 2, 4, 2, 3, 0, 0, 0, 3, 2, 4,\n",
      "        0, 4, 5, 0, 2, 5, 3, 1, 4, 2, 5, 4, 3, 2, 4, 4, 3, 4, 3, 6, 4, 4, 0, 0,\n",
      "        3, 3, 4, 3, 6, 2, 3, 4, 5, 6, 3, 3, 0, 4, 6, 4])\n",
      "image vector (x): \n",
      " tensor([[ 0.9294,  0.5765, -0.2549, -0.1059, -0.1294],\n",
      "        [ 0.9137,  0.7020, -0.0196, -0.0431, -0.1137],\n",
      "        [ 0.8980,  0.6784, -0.0431,  0.0824, -0.1216],\n",
      "        [ 0.9373,  0.5529, -0.2627,  0.0353, -0.2471],\n",
      "        [ 0.9608,  0.5294, -0.5216, -0.1529, -0.3882]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Labels values (y): \\n {labels}\")\n",
    "print(f\"image vector (x): \\n {images[0,0,:5,:5]}\") # mostra os primeiros 5x5 pixels da primeira imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4360813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val(train_dataset, val_ratio, batch_size, seed):\n",
    "    train_size = int((1 - val_ratio) * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "\n",
    "    train_subset, val_subset = random_split(train_dataset, [train_size, val_size], \n",
    "                                            generator = torch.Generator().manual_sedd(seed))\n",
    "\n",
    "    train = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e84bdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do model CNN\n",
    "num_emotions = 7\n",
    "lr = 0.0005\n",
    "num_epochs = 300\n",
    "dropout = 0.1 \n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c821d0be",
   "metadata": {},
   "source": [
    "out_channels = nº filtros\n",
    "kenerl = tamanho do filtro\n",
    "filtro = conjunto de pesos aprendidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3baf7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    #def __init__(self, num_classes = num_emotions):\n",
    "    def __init__(self, num_classes = num_emotions, dropout_prob = dropout): #AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
    "        super(CNN, self).__init__()\n",
    "        #1ª camada convolucional\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)   # (1,48,48) → (16,48,48)\n",
    "        #2ª camada convolucional\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)  # (16,24,24) → (32,24,24)                         \n",
    "        self.pool = nn.MaxPool2d(2,2)  # halves spatial size\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        #fully connected\n",
    "        self.fc1 = nn.Linear(32 * 12 * 12, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):      # input x shape: (batch_size, 1, 48, 48)\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # conv1 + relu + pool: (batch,16,48,48) → (batch,16,24,24)\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # conv2 + relu + pool: (batch,32,24,24) → (batch,32,12,12)\n",
    "        x = torch.flatten(x, 1)               # flatten except batch dim\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)                       #output logits\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb18b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, Loss, Optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac929ffb",
   "metadata": {},
   "source": [
    "Bons candidatos para tuning:\n",
    "\n",
    "-lr\n",
    "-dropout\n",
    "-nº de filtros por layer\n",
    "-nº de camadas\n",
    "\n",
    "\n",
    "-kernel size (às vezes)????? + padding tem valores padrao é ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc2bcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CROSS VALIDATION\n",
    "def cross_validation_cnn(\n",
    "    train_dataset,\n",
    "    num_emotions,\n",
    "    device,\n",
    "    k_folds=5,\n",
    "    patience = 5,\n",
    "    batch_size=64,\n",
    "    num_epochs=50,\n",
    "    lr=1e-3,\n",
    "    dropout_prob= 0.1,\n",
    "):\n",
    "    fold_results = []\n",
    "    \n",
    "    for run in range(k_folds):\n",
    "        print(f\"\\n===== Fold {run+1}/{k_folds} =====\")\n",
    "        \n",
    "        #split train - val\n",
    "        seed = 42 + run   #“Different random seeds were used at each fold to ensure variability in the train–validation splits.”\n",
    "        val_ratio = 0.2\n",
    "\n",
    "        train, val = split_train_val(train_dataset, val_ratio, batch_size, seed)\n",
    "\n",
    "    \n",
    "        #MODEL (novo em cada k-fold)\n",
    "        model = CNN (num_classes = num_emotions, dropout_prob = dropout_prob).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()    #for multi-class classification\n",
    "        optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "        #early stopping (por fold)\n",
    "        best_val_loss = float(\"inf\") #infinito positivo, para ser maior que qualquer loss possível\n",
    "        best_val_acc = 0\n",
    "        counter = 0\n",
    "\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            #----TRAIN----\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for images, labels in train:  #images = batch_x, labels = batch_y\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                logits = model(images)    #logits =outputs\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item() * images.size(0)\n",
    "                _, preds = torch.max(logits, 1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "            train_loss /= total\n",
    "            train_acc = correct / total\n",
    "        \n",
    "            #----VALIDATION----\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                    logits = model(images)\n",
    "                    loss = criterion(logits, labels)\n",
    "\n",
    "                    val_loss += loss.item() * images.size(0)\n",
    "                    _, preds = torch.max(logits, 1)\n",
    "                    correct += (preds == labels).sum().item()\n",
    "                    total += labels.size(0)\n",
    "\n",
    "                val_loss /= total\n",
    "                val_acc = correct / total\n",
    "\n",
    "                print(\n",
    "                    f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "                    f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} \"\n",
    "                    f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "                #----EARLY STOPPING----  \n",
    "                if val_loss < best_val_loss:\n",
    "                   best_val_loss = val_loss\n",
    "                   best_val_acc = val_acc\n",
    "                   counter = 0\n",
    "            \n",
    "                else:\n",
    "                    counter += 1\n",
    "                    if counter >= patience:  #5 = patience-> hyperparameter\n",
    "                       print(\"Early stopping triggered\")\n",
    "                       break\n",
    "\n",
    "        fold_results.append(best_val_acc)\n",
    "    \n",
    "    mean_acc = sum(fold_results) / len(fold_results)\n",
    "    print(f\"\\nMean validation accuracy over {k_folds} folds: {mean_acc:.4f}\")\n",
    "\n",
    "    return mean_acc     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c618c5d",
   "metadata": {},
   "source": [
    "na função de cima não estou a guardar o best model por fold apenas o best-val_acc.... os pesos não ficam guardados. ok para tuning mas para o futuro temos de ver se não tenho de acrescentar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deefdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters tunning\n",
    "for lr in [0.001, 0.005, 1e-4]:\n",
    "    for dropout in [0.0, 0.1, 0.2]:\n",
    "        mean_acc = cross_validation_cnn(\n",
    "            train_dataset,\n",
    "            num_emotions,\n",
    "            device,\n",
    "            lr=lr,\n",
    "            dropout_prob =dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ccac04",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#TESTE THE MODEL\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images_x, labels_y in test:\n",
    "        images_x = images_x.to(device)\n",
    "        labels_y = labels_y.to(device)\n",
    "        logits = model(images_x)\n",
    "        _, preds = torch.max(logits, 1)\n",
    "        \n",
    "        #all_preds.extend(preds.cpu().numpy())    -- converte para numpy pior se quiser continuar a usar pytorch/GPU\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(labels)  \n",
    "\n",
    "# concatenar todos os batches em um tensor só\n",
    "all_preds = torch.cat(all_preds).cpu()\n",
    "all_labels = torch.cat(all_labels).cpu()\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b08375f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#CLASSIFICATION REPORT (stor way)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1a30a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#CLASSIFICATION REPORT (CHAT WAY)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=[\n",
    "    'Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral'\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c949cf5",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#FUZZY PART------------------------------------------------------\n",
    "\n",
    "probs = torch.softmax(logits, dim = 1)  # [batc_size, 7]\n",
    "#CRIAR ALGO GENERICO QUE DÉ PARA TODAS AS EMOÇOEES SEM TER QUE ESCREVR UMA POR UMA\n",
    "'Obs: mais avançado seria usar trapezoidal ou triangular membership functions, mas para o projeto de aula isto já é suficiente para interpretabilidade'\n",
    "\n",
    "def fuzzy_happy(prob):\n",
    "    if prob < 0.3:\n",
    "        return \"slightly happy\"\n",
    "    elif prob < 0.7:\n",
    "        return \"moderately happy\"\n",
    "    else:\n",
    "        return \"highly happy\"\n",
    "\n",
    "# probs → saída softmax da CNN\n",
    "softmax_probs = torch.softmax(logits, dim=1).cpu().numpy()[0]  # exemplo de 1 imagem\n",
    "\n",
    "labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "fuzzy_labels = []\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    p = softmax_probs[i]\n",
    "    if label == 'Happy':\n",
    "        fuzzy_labels.append(fuzzy_happy(p))\n",
    "    # repetir para outras emoções com suas funções fuzzy\n",
    "\n",
    "print(fuzzy_labels)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced-automation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
