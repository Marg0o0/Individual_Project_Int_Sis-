{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2c0843bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e0ac04",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#ativar no google colab\n",
    "#!unzip fer2013.zip "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f806e421",
   "metadata": {},
   "source": [
    "sempre que corro esta célula a outra mais abaixo com os valores de labels troca os resultados.... preocupante?!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "977c72ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRE-PROCESSAMENTO\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),         #garante que as imagens são a preto e branco\n",
    "    transforms.Resize((48, 48)),    #garante que têm dimensão 48x48\n",
    "    transforms.ToTensor(),          #converte para tensores pytorch (1,48,48)\n",
    "    transforms.Normalize((0.5,), (0.5,))  # normaliza pixels entre -1 e 1 (mean = 0.5, std = 0.5)\n",
    "])\n",
    "\n",
    "# \"SPLIT\" DATASET (definimos o treino e o teste):\n",
    "#ImageFolder → usa os nomes das pastas como labels\n",
    "train_dataset = datasets.ImageFolder( root=\"fer2013/train\", transform=transform)\n",
    "test_dataset = datasets.ImageFolder( root=\"fer2013/test\", transform=transform)\n",
    "\n",
    "#LOAD data\n",
    "train_loader = DataLoader( train_dataset, batch_size=64, shuffle=True)  #---------------------------apagavel\n",
    "test = DataLoader( test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb67832",
   "metadata": {},
   "source": [
    "labels correspondem ás emoçoes ou seja\n",
    "\n",
    "imagens -> x\n",
    "\n",
    "labels -> y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eee76520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 48, 48])\n",
      "torch.Size([64])\n",
      "tensor(-1.) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "#VERIFICAR SE ESTÁ TUDO OK--------------------------------------apagavel\n",
    "images, labels = next(iter(train_loader))\n",
    "print(images.shape)  # deve dar: [64, 1, 48, 48]\n",
    "print(labels.shape)  # deve dar: [64]\n",
    "print(images.min(), images.max())  # já está normalizado para -1 a 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6472f04d",
   "metadata": {},
   "source": [
    "0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3740a3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels values (y): \n",
      " tensor([5, 0, 6, 4, 0, 6, 0, 3, 3, 5, 0, 0, 3, 0, 2, 0, 5, 3, 3, 2, 2, 4, 3, 3,\n",
      "        0, 2, 4, 3, 3, 3, 4, 5, 4, 6, 0, 3, 3, 3, 5, 5, 3, 5, 6, 2, 3, 4, 3, 4,\n",
      "        3, 3, 6, 3, 6, 6, 5, 5, 2, 3, 3, 3, 5, 0, 1, 6])\n",
      "image vector (x): \n",
      " tensor([[0.8353, 0.8353, 0.7255, 0.6784, 0.6941],\n",
      "        [0.8353, 0.8118, 0.7176, 0.6863, 0.6941],\n",
      "        [0.8431, 0.7961, 0.7176, 0.7020, 0.6941],\n",
      "        [0.8510, 0.7804, 0.7333, 0.7176, 0.6784],\n",
      "        [0.8353, 0.7569, 0.7333, 0.7098, 0.6549]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Labels values (y): \\n {labels}\")\n",
    "print(f\"image vector (x): \\n {images[0,0,:5,:5]}\") # mostra os primeiros 5x5 pixels da primeira imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54442c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MONTE CARLO CV---------------------------------------------\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def split_train_val_stratified_wrong(train_dataset, k_folds, val_ratio, batch_size, seed):\n",
    "    targets = train_dataset.targets  #lista de labels (0, 1, 2, ....)\n",
    "\n",
    "    skf = StratifiedShuffleSplit(\n",
    "        n_splits=k_folds,\n",
    "        test_size=val_ratio,\n",
    "        random_state=seed,\n",
    "        shuffle=True)\n",
    "\n",
    "    train_idx, val_idx = next(skf.split(np.zeros(len(targets)), targets))\n",
    "\n",
    "    train_subset = Subset(train_dataset, train_idx)\n",
    "    val_subset   = Subset(train_dataset, val_idx)\n",
    "\n",
    "    train = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    val = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    return train, val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "968258ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VERDADEIRO K-FOLD----------------------------------------------\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def split_train_val_stratified(train_dataset, k_folds, batch_size, seed):\n",
    "    targets = np.array(train_dataset.targets)  #labels\n",
    "\n",
    "    skf = StratifiedKFold(\n",
    "        n_splits=k_folds,\n",
    "        shuffle=True,\n",
    "        random_state=seed)\n",
    "\n",
    "    loaders = []\n",
    "\n",
    "    for train_idx, val_idx in skf.split(np.zeros(len(targets)), targets):\n",
    "        train_subset = Subset(train_dataset, train_idx)\n",
    "        val_subset   = Subset(train_dataset, val_idx)\n",
    "\n",
    "        train = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "        val= DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "        loaders.append((train, val))\n",
    "\n",
    "    return loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0e84bdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do model CNN\n",
    "num_emotions = 7\n",
    "lr = 0.001\n",
    "num_epochs = 100\n",
    "dropout = 0.1 \n",
    "batch_size = 64\n",
    "k_folds = 3\n",
    "patience =10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c821d0be",
   "metadata": {},
   "source": [
    "out_channels = nº filtros\n",
    "kenerl = tamanho do filtro\n",
    "filtro = conjunto de pesos aprendidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c3baf7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    #def __init__(self, num_classes = num_emotions):\n",
    "    def __init__(self, num_classes = num_emotions, dropout_prob = dropout): \n",
    "        super(CNN, self).__init__()\n",
    "        #1ª camada convolucional\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)   # (1,48,48) → (16,48,48)\n",
    "        #2ª camada convolucional\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)  # (16,24,24) → (32,24,24)                         \n",
    "        #3ª camada convolucional\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)  # (32,12,12) → (64,12,12)\n",
    "        self.pool = nn.MaxPool2d(2,2)  # halves spatial size\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        #fully connected\n",
    "        self.fc1 = nn.Linear(64 * 6 * 6, 256)  # calcula o tamanho correto: 64 filtros, 6x6 depois do pooling\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):      # input x shape: (batch_size, 1, 48, 48)\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # conv1 + relu + pool: (batch,16,48,48) → (batch,16,24,24)\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # conv2 + relu + pool: (batch,32,24,24) → (batch,32,12,12)\n",
    "        x = self.pool(F.relu(self.conv3(x)))  # conv3 + relu + pool: (batch,64,12,12) → (batch,64,6,6)\n",
    "        x = torch.flatten(x, 1)     # flatten except batch dim\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)       #output logits\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac929ffb",
   "metadata": {},
   "source": [
    "Bons candidatos para tuning:\n",
    "\n",
    "-lr\n",
    "-dropout\n",
    "-nº de filtros por layer\n",
    "-nº de camadas\n",
    "\n",
    "\n",
    "-kernel size (às vezes)????? + padding tem valores padrao é ok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d089578",
   "metadata": {},
   "source": [
    "-Repeated Hold-Out\n",
    "\n",
    "-Monte Carlo Cross-Validation\n",
    "\n",
    "como se pode chamar ao tipo de cross validation que estou a fazer, pois uso um valor de seed variavel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bfc2bcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CROSS VALIDATION\n",
    "def cross_validation_cnn(\n",
    "    train_dataset,\n",
    "    num_emotions,\n",
    "    device,\n",
    "    k_folds = k_folds,\n",
    "    patience = patience,\n",
    "    batch_size= batch_size,\n",
    "    num_epochs = num_epochs,\n",
    "    lr = lr,\n",
    "    dropout_prob= dropout,\n",
    "):\n",
    "    fold_results = []\n",
    "    \n",
    "    #opção certa de k-fold verdadeiro\n",
    "    fold_loaders = split_train_val_stratified(train_dataset, k_folds, batch_size, seed = 42)\n",
    "    \n",
    "    for fold, (train, val) in enumerate(fold_loaders):\n",
    "        print(f\"\\n===== Fold {fold+1}=====\")\n",
    "    #for run in range(k_folds):\n",
    "        #print(f\"\\n===== Fold {run+1}/{k_folds} =====\")\n",
    "        \n",
    "        #split train  val\n",
    "        #seed = 42 + run   #“Different random seeds were used at each fold to ensure variability in the train–validation splits.”\n",
    "        #val_ratio = 0.2\n",
    "\n",
    "        \n",
    "        \n",
    "        #opção com stratify MONTE CARLO\n",
    "        #train, val = split_train_val_stratified_wrong(train_dataset, k_folds, val_ratio, batch_size=batch_size, seed=seed)\n",
    "\n",
    "    \n",
    "        #MODEL (novo em cada k-fold)\n",
    "        model = CNN (num_classes = num_emotions, dropout_prob = dropout_prob).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()    #for multi-class classification\n",
    "        optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "        #early stopping (por fold)\n",
    "        best_val_loss = float(\"inf\") #infinito positivo, para ser maior que qualquer loss possível\n",
    "        best_val_acc = 0\n",
    "        counter = 0\n",
    "\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            #----TRAIN----\n",
    "            model.train()\n",
    "            #train_loss=0\n",
    "            correct, total = 0, 0\n",
    "                        \n",
    "            for images, labels in train:  #images = batch_x, labels = batch_y\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                logits = model(images)    #logits =outputs\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                #train_loss += loss.item() * images.size(0)\n",
    "\n",
    "                _, preds = torch.max(logits, 1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "            #train_loss /= total\n",
    "            #train_acc = correct / total\n",
    "        \n",
    "            #----VALIDATION----\n",
    "            model.eval()\n",
    "            val_loss, val_correct, val_total = 0, 0, 0\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                    logits = model(images)\n",
    "                    loss = criterion(logits, labels)\n",
    "\n",
    "                    val_loss += loss.item() * images.size(0)\n",
    "                    _, preds = torch.max(logits, 1)\n",
    "                    val_correct += (preds == labels).sum().item()\n",
    "                    val_total += labels.size(0)\n",
    "\n",
    "                val_loss /= val_total\n",
    "                val_acc = val_correct / val_total\n",
    "\n",
    "                print(\n",
    "                    f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "                    #f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} \"\n",
    "                    f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "                #----EARLY STOPPING----  \n",
    "                if val_loss < best_val_loss:\n",
    "                   best_val_loss = val_loss\n",
    "                   best_val_acc = val_acc\n",
    "                   counter = 0\n",
    "            \n",
    "                else:\n",
    "                    counter += 1\n",
    "                    if counter >= patience:  \n",
    "                       print(\"Early stopping triggered\")\n",
    "                       break\n",
    "\n",
    "        fold_results.append(best_val_acc)\n",
    "    \n",
    "    mean_acc = sum(fold_results) / len(fold_results)\n",
    "    print(f\"\\nMean validation accuracy over {k_folds} folds: {mean_acc:.4f}\")\n",
    "\n",
    "    return mean_acc     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c618c5d",
   "metadata": {},
   "source": [
    "na função de cima não estou a guardar o best model por fold apenas o best-val_acc.... os pesos não ficam guardados. ok para tuning mas para o futuro temos de ver se não tenho de acrescentar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8deefdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing lr=0.001, dropout=0.0\n",
      "\n",
      "===== Fold 1=====\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dropout \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.2\u001b[39m]:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTesting lr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, dropout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdropout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m     mean_acc \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validation_cnn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_emotions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk_folds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m#no anterior temos tres par acelerar\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropout_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mean_acc \u001b[38;5;241m>\u001b[39m best_overall_acc:\n\u001b[0;32m     19\u001b[0m         best_overall_acc \u001b[38;5;241m=\u001b[39m mean_acc\n",
      "Cell \u001b[1;32mIn[88], line 50\u001b[0m, in \u001b[0;36mcross_validation_cnn\u001b[1;34m(train_dataset, num_emotions, device, k_folds, patience, batch_size, num_epochs, lr, dropout_prob)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m#train_loss=0\u001b[39;00m\n\u001b[0;32m     48\u001b[0m correct, total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m train:  \u001b[38;5;66;03m#images = batch_x, labels = batch_y\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     53\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(images)    \u001b[38;5;66;03m#logits =outputs\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Margarida\\miniconda3\\envs\\advanced-automation\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:484\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Margarida\\miniconda3\\envs\\advanced-automation\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:415\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Margarida\\miniconda3\\envs\\advanced-automation\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1138\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1131\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1138\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\Margarida\\miniconda3\\envs\\advanced-automation\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Margarida\\miniconda3\\envs\\advanced-automation\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Margarida\\miniconda3\\envs\\advanced-automation\\lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Margarida\\miniconda3\\envs\\advanced-automation\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Margarida\\miniconda3\\envs\\advanced-automation\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#hyperparameters tunning\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "best_overall_acc = 0\n",
    "best_hyperparams = {}\n",
    "\n",
    "for lr in [0.001, 1e-4]:\n",
    "    for dropout in [0.0, 0.2]:\n",
    "        print(f\"\\nTesting lr={lr}, dropout={dropout}\")\n",
    "        mean_acc = cross_validation_cnn(\n",
    "            train_dataset,\n",
    "            num_emotions,\n",
    "            device,\n",
    "            k_folds= k_folds,       #no anterior temos tres par acelerar\n",
    "            lr=lr,\n",
    "            dropout_prob =dropout)\n",
    "        \n",
    "        if mean_acc > best_overall_acc:\n",
    "            best_overall_acc = mean_acc\n",
    "            best_hyperparams = {\"lr\": lr, \"dropout\": dropout}\n",
    "\n",
    "print(\"\\nBest mean validation accuracy:\", best_overall_acc)\n",
    "print(\"Best hyperparameters:\", best_hyperparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ad5913",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#Final training with best hyperparameter\n",
    "#----------------------------------------------\n",
    "# TREINO FINAL COM MELHORES HYPERPARAMETERS--------por rever-----------------\n",
    "#----------------------------------------------\n",
    "\n",
    "final_model = CNN(num_classes=num_emotions, dropout_prob=best_hyperparams['dropout']).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=best_hyperparams['lr'])\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    final_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = final_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# salvar modelo final\n",
    "torch.save(final_model.state_dict(), \"best_cnn_model.pth\")\n",
    "print(\"Final model saved as 'best_cnn_model.pth'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3bbb35",
   "metadata": {},
   "source": [
    "após treinar no google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2871190d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"best_model.pth\", map_location=device))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ccac04",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#TESTE THE MODEL\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images_x, labels_y in test:\n",
    "        images_x = images_x.to(device)\n",
    "        labels_y = labels_y.to(device)\n",
    "        logits = model(images_x)\n",
    "        _, preds = torch.max(logits, 1)\n",
    "        \n",
    "        #all_preds.extend(preds.cpu().numpy())    -- converte para numpy pior se quiser continuar a usar pytorch/GPU\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(labels)  \n",
    "\n",
    "# concatenar todos os batches em um tensor só\n",
    "all_preds = torch.cat(all_preds).cpu()\n",
    "all_labels = torch.cat(all_labels).cpu()\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b08375f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#CLASSIFICATION REPORT (stor way)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1a30a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#CLASSIFICATION REPORT (CHAT WAY)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=[\n",
    "    'Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral'\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c949cf5",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#FUZZY PART------------------------------------------------------\n",
    "\n",
    "probs = torch.softmax(logits, dim = 1)  # [batc_size, 7]\n",
    "#CRIAR ALGO GENERICO QUE DÉ PARA TODAS AS EMOÇOEES SEM TER QUE ESCREVR UMA POR UMA\n",
    "'Obs: mais avançado seria usar trapezoidal ou triangular membership functions, mas para o projeto de aula isto já é suficiente para interpretabilidade'\n",
    "\n",
    "def fuzzy_happy(prob):\n",
    "    if prob < 0.3:\n",
    "        return \"slightly happy\"\n",
    "    elif prob < 0.7:\n",
    "        return \"moderately happy\"\n",
    "    else:\n",
    "        return \"highly happy\"\n",
    "\n",
    "# probs → saída softmax da CNN\n",
    "softmax_probs = torch.softmax(logits, dim=1).cpu().numpy()[0]  # exemplo de 1 imagem\n",
    "\n",
    "labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "fuzzy_labels = []\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    p = softmax_probs[i]\n",
    "    if label == 'Happy':\n",
    "        fuzzy_labels.append(fuzzy_happy(p))\n",
    "    # repetir para outras emoções com suas funções fuzzy\n",
    "\n",
    "print(fuzzy_labels)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced-automation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
